{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1790ced",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"height: 2px; background: linear-gradient(to right, #E31B1D 50%, #00A4DD 50%);\">\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "    \n",
    "<div style=\"width: 20%; text-align: left;\">\n",
    "    <img src=\"logos/hpc_logo.png\" alt=\"HPC logo\" width=\"100px\">\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 60%; text-align: center;\">\n",
    "    <strong><center><font size = \"6\">Array Operations with Python parallel packages in a single node</font></center></strong>\n",
    "    <br>\n",
    "    <strong><center><font size = \"4\">Python + HPC</font></center></strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"width: 20%; text-align: right;display: flex; justify-content: center;align-items: center;\">\n",
    "    <img src=\"logos/unilu_logo.png\" alt=\"UL logo\" width=\"100px\">\n",
    "</div>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr style=\"height: 2px; background: linear-gradient(to right, #E31B1D 50%, #00A4DD 50%);\">\n",
    "\n",
    "By: **Oscar J. CASTRO-LOPEZ** (oscar.castro@uni.lu)\n",
    "\n",
    "**University of Luxembourg | HPC | PCOG**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "0. [Workshop Overview](#workshopoverview)\n",
    "1. [Use case: Euclidean distance](#usecase)\n",
    "2. [Python Baseline](#python)\n",
    "3. [Numpy Implementation](#numpy)\n",
    "4. [Python Multiprocessing](#multiproc)\n",
    "5. [PyMP](#pymp)\n",
    "6. [Cython](#cython)\n",
    "7. [Numba](#numba)\n",
    "8. [Benchmarking](#benchmarking)\n",
    "9. [Conclusion](#conclusions)\n",
    "\n",
    "\n",
    "## 0. Workshop Overview <a name=\"workshopoverview\"></a>\n",
    "\n",
    "Welcome to the **Python + HPC workshop**. In this interactive session, we will start to explore high-performance computing in Python. We'll optimize Python code using the Euclidean distance measurement algorithm, showcasing the efficiency of map multiprocessing, PyMP, Cython, and Numba versus Numpy. By the end, you'll master Python's potential for high-performance tasks in a **single node**.\n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "Before we begin, please make sure you have the following:\n",
    "\n",
    "- A basic understanding of Python programming.\n",
    "- Familiarity with Jupyter Notebook (installed and configured). \n",
    "- A basic understanding of Numpy and linear algebra.\n",
    "- Familiarity of parallel computing.\n",
    "\n",
    "### Agenda\n",
    "\n",
    "1. **Use case overview**\n",
    "2. **Python baseline**\n",
    "2. **Numpy implementation**\n",
    "2. **Python Multiprocessing**\n",
    "3. **PyMP - OpenMP-like**\n",
    "3. **Cython implementation**\n",
    "3. **Numba implementation**\n",
    "3. **Benchmarking**\n",
    "3. **Conclusion**\n",
    "\n",
    "### Workshop Key Goals\n",
    "The primary objectives of this workshop are:\n",
    "\n",
    "- To provide a basic understanding of parallelizing python code.\n",
    "- To equip you with practical skills on how to efficiently improve Python code with parallel approach.\n",
    "- Give a general overview of parallelization tools available in Python's ecosystem.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "To get started with this workshop, follow these steps:\n",
    "\n",
    "1. Clone or download the workshop materials from the [GitHub repository](https://github.com/ULHPC/python-school).\n",
    "2. Open a terminal and navigate to the workshop directory.\n",
    "3. Open this notebook (`3_Edistance_singlenode_parallel.ipynb`) in your browser.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<hr style=\"height: 2px; background: linear-gradient(to right, #E31B1D 50%, #00A4DD 50%);\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a13a1ca",
   "metadata": {},
   "source": [
    "## 1. Use Case: Euclidean Distance <a name=\"usecase\"></a>\n",
    "    \n",
    "In this notebook, the Euclidean distance measurement algorithm is employed as an illustrative example to showcase the effective utilization of map multiprocessing against Numpy.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Euclidean_distance\n",
    "\n",
    "The Euclidean distance between two points P = (p1, p2, ..., pn) and Q = (q1, q2, ..., qn) in n-dimensional space is calculated using the following formula:\n",
    "\n",
    "$$d(P, Q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \\ldots + (p_n - q_n)^2}$$\n",
    "\n",
    "In the formula, $d(P, Q)$ represents the Euclidean distance between points $P$ and $Q$, $\\sqrt{\\ldots}$ denotes the square root operation, and $(p1 - q1)^2$ represents the squared difference between the corresponding coordinates of the points.\n",
    "\n",
    "### Generate random data for testing\n",
    "\n",
    "The following code generates two matrices of random numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38209d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rows = 500\n",
    "cols = 500\n",
    "np.random.seed(699)\n",
    "A = np.random.rand(rows*cols).reshape((rows,cols))\n",
    "B = np.random.rand(rows*cols).reshape((rows,cols))\n",
    "print(f'A shape {A.shape}')\n",
    "print(f'B shape {A.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797a5ab",
   "metadata": {},
   "source": [
    "## 2. Python baseline implementation <a name=\"python\"></a>\n",
    "\n",
    "To facilitate understanding the implementation a first step-by-step function using only Python is presented. Three loops are required:\n",
    "\n",
    "- The first loop is used to iterate over each row of matrix1. \n",
    "- The second loop is used to iterate over each row of matrix2. \n",
    "- The third and most inner loop iterates over the columns (both matrices must have the same number of columns). \n",
    "\n",
    "In the inner loop is where the operations difference, squared and sum of values is done. \n",
    "\n",
    "Each time the third loop ends, the `sqrt` function is applied to the result and assigned to its index on the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61948d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def edistance_python(matrix1, matrix2):\n",
    "    output = np.zeros((matrix1.shape[0], matrix2.shape[0]))\n",
    "    for i in range(matrix1.shape[0]): # Loop over matrix1 rows\n",
    "        for j in range(matrix2.shape[0]): # Loop over matrix2 rows\n",
    "            sum_squared_diff = 0.0\n",
    "            for k in range(matrix1.shape[1]): # Loop over two matrices columns\n",
    "                diff = matrix1[i, k] - matrix2[j, k] # Get difference\n",
    "                squared_diff = diff ** 2 # Get squared difference\n",
    "                sum_squared_diff += squared_diff # Sum results\n",
    "            output[i, j] = sqrt(sum_squared_diff) # Square root\n",
    "    return output\n",
    "\n",
    "result_python = edistance_python(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa02b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_python = %timeit -n1 -r1 -o result_python = edistance_python(A, B) \n",
    "#time_python = %timeit -o result_python = edistance_python(A, B) #this takes longer,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85af54e",
   "metadata": {},
   "source": [
    "## 3. Numpy implementation <a name=\"numpy\"></a>\n",
    "\n",
    "In this implementation, we use a loop to iterate over each row of matrix1. Inside the loop we apply all the operations of the Euclidian distance (as stated in the aforementioned equation). Thanks to broadcasting arrays principle used by Numpy we can avoid using another loop and we can make operations between a row and a matrix to make these operations. In the case of the ``np.sum()`` function we use ``axis=1`` so that the operation is between one row of matrix1 and all the rows of matrix2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45409adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edistance_numpy(matrix1, matrix2):\n",
    "    output = np.zeros((matrix1.shape[0], matrix2.shape[0]))\n",
    "    for i, row1 in enumerate(matrix1): # Loop over matrix1 rows\n",
    "        output[i] = np.sqrt(np.sum((row1 - matrix2)**2, axis=1))\n",
    "    return output\n",
    "\n",
    "result_numpy = edistance_numpy(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eaccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_numpy = %timeit -n1 -r1 -o result_numpy = edistance_numpy(A, B)\n",
    "time_numpy = %timeit -o result_numpy = edistance_numpy(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041c000",
   "metadata": {},
   "source": [
    "## 4. Python's Multiprocessing <a name=\"multiproc\"></a>\n",
    "\n",
    "From https://docs.python.org/3/library/multiprocessing.html\n",
    "\n",
    "Multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.\n",
    "\n",
    "Basic example:\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        print(p.map(f, [1, 2, 3]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c11c63",
   "metadata": {},
   "source": [
    "This code demonstrates the use of the multiprocessing module in Python to perform parallel processing using a pool of worker processes. Here's a breakdown of what the code does:\n",
    "\n",
    "- Define a function f(x) that takes an input x and returns the square of x. \n",
    "- Create a Pool object p with 5 worker processes using the with statement. The number 5 specifies the maximum number of worker processes to be used concurrently.\n",
    "- Use the p.map() method to distribute the function f to the worker processes with a list of inputs [1, 2, 3]. The map function takes the list of inputs and applies the function f to each input element in parallel.\n",
    "- Print the result of p.map(f, [1, 2, 3]). The map function returns a list of results, in this case, the squares of the input numbers, which are [1, 4, 9].\n",
    "\n",
    "In summary, this code creates a pool of 5 worker processes and uses them to apply the f function to a list of numbers in parallel, calculating the squares of those numbers. The result is printed to the console. This is a simple example of parallel processing to illustrate how the multiprocessing module can be used to distribute tasks across multiple processes for improved performance.\n",
    "\n",
    "The following code is a basic example of using pool of multiprocessing. We have a function that multiplies the input and prints the input, result and date-time. Then, we have another function that creates an array and inside a pool block we use map to apply the function to all the values of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def f(x):\n",
    "    result = x*x\n",
    "    time.sleep(1) # emulate long runtime\n",
    "    print(f'Input: {x}| Result: {result}| Finish: {datetime.datetime.now()}\\n')\n",
    "    return result\n",
    "\n",
    "def test_mp(n=10):\n",
    "    arr = [x for x in range(n)]\n",
    "    with Pool(5) as p:\n",
    "        print(p.map(f, arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ea504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76c13",
   "metadata": {},
   "source": [
    "The final result is the same. However, the order of computing is always different.\n",
    "\n",
    "The Pool class has different methods which allows tasks to be offloaded to the worker processes in a few different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03cf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, TimeoutError\n",
    "import time\n",
    "import os\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "with Pool(processes=4) as pool:\n",
    "\n",
    "    # print \"[0, 1, 4,..., 81]\"\n",
    "    print(pool.map(f, range(10)))\n",
    "    print('--------------------------------')\n",
    "    # print same numbers in arbitrary order\n",
    "    for i in pool.imap_unordered(f, range(10)):\n",
    "        print(i)\n",
    "    print('--------------------------------')\n",
    "    # evaluate \"f(20)\" asynchronously\n",
    "    res = pool.apply_async(f, (20,))      # runs in *only* one process\n",
    "    print(res.get(timeout=1))             # prints \"400\"\n",
    "    print('--------------------------------')\n",
    "    # evaluate \"os.getpid()\" asynchronously\n",
    "    res = pool.apply_async(os.getpid, ()) # runs in *only* one process\n",
    "    print(res.get(timeout=1))             # prints the PID of that process\n",
    "    print('--------------------------------')\n",
    "    # launching multiple evaluations asynchronously *may* use more processes\n",
    "    multiple_results = [pool.apply_async(os.getpid, ()) for i in range(4)]\n",
    "    print([res.get(timeout=1) for res in multiple_results])\n",
    "    print('--------------------------------')\n",
    "    # make a single worker sleep for 10 seconds\n",
    "    res = pool.apply_async(time.sleep, (10,))\n",
    "    try:\n",
    "        print(res.get(timeout=1))\n",
    "    except TimeoutError:\n",
    "        print(\"We lacked patience and got a multiprocessing.TimeoutError\")\n",
    "\n",
    "    print(\"For the moment, the pool remains available for more work\")\n",
    "\n",
    "# exiting the 'with'-block has stopped the pool\n",
    "print(\"Now the pool is closed and no longer available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa0f75",
   "metadata": {},
   "source": [
    "From the Python documentation:\n",
    "\n",
    "**map** A parallel equivalent of the map() built-in function (it supports only one iterable argument though, for multiple iterables see starmap()). It blocks until the result is ready.\n",
    "\n",
    "**imap**\n",
    "A lazier version of map(). The chunksize argument is the same as the one used by the map() method. For very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1.\n",
    "\n",
    "**imap_unordered**\n",
    "The same as imap() except that the ordering of the results from the returned iterator should be considered arbitrary. (Only when there is only one worker process is the order guaranteed to be \"correct\".)\n",
    "\n",
    "**apply_async**\n",
    "A variant of the apply() method which returns a AsyncResult object.\n",
    "- If callback is specified then it should be a callable which accepts a single argument. When the result becomes ready callback is applied to it, that is unless the call failed, in which case the error_callback is applied instead.\n",
    "- If error_callback is specified then it should be a callable which accepts a single argument. If the target function fails, then the error_callback is called with the exception instance.\n",
    "- Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "We can use multiprocessing to parallelize the code of our previous function `edistance_np`. Instead of using a loop we can use a pool of processes where each process works with one row. For this we define a new function that works only with one row and the matrix.\n",
    "\n",
    "- First, we load the libraries for multiprocessing.\n",
    "- We use the logic of the Numpy function where we process a row and matrix inside a loop. But in this case, map is going to replace the loop.\n",
    "- We define an auxiliary function which is going to be called in parallel. This function, receives one parameter which is a pair/tuple of values. Inside the function, the parameter is assigned to two variables: row and matrix. This can be done similarly with `partial` or with `starmap`.\n",
    "- We define the main function that receives the two matrices and inside a `with` block we call the `pool.map()` function that receives the name of the function and a list comprehension that creates a list of pairs with rows and matrix.\n",
    "- Multiprocessing map function only works with iterable types. \n",
    "- To be consistent with the previous result we convert the result which is a list of lists into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab60fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def edistance_mp(args):\n",
    "    row, matrix = args\n",
    "    return np.sqrt(np.sum((row - matrix) ** 2, axis=1))\n",
    "\n",
    "def edistance_multiproc(matrix1, matrix2):\n",
    "    n_processes = 5\n",
    "    with mp.Pool(n_processes) as pool:\n",
    "        # Pass the function and an iterable - This return a list of lists\n",
    "        # Complete the function\n",
    "        \n",
    "    return np.array(results) # Convert the list to a Numpy array\n",
    "\n",
    "result_multiproc = edistance_multiproc(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_multiproc = %timeit -n1 -r1 -o result_multiproc = edistance_multiproc(A, B)\n",
    "time_multiproc = %timeit -o result_multiproc = edistance_multiproc(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ca585",
   "metadata": {},
   "source": [
    "## 5. PyMP - OpenMP-like library <a name=\"pymp\"></a>\n",
    "\n",
    "PyMP is a library that provides OpenMP-like functionality for parallel programming in Python. It allows you to write parallel code that can take advantage of multiple CPU cores. PyMP is designed to make it easier to parallelize loops in Python programs, similar to how OpenMP simplifies parallel programming in C and C++.\n",
    "\n",
    "https://github.com/classner/pymp\n",
    "\n",
    "**Install the library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc442c-8587-49c5-8acb-d2becddfa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymp-pypi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7194666",
   "metadata": {},
   "source": [
    "The following Python code defines a function `f(x)` that calculates the square of a given input x and prints information about the input, result, and the current time.\n",
    "\n",
    "Then, we test function `f(x)` by utilizing the Pymp library for parallel computing. It creates an array arr of numbers from 0 to n-1 and a shared memory array output using Pymp. The script then runs a parallel loop using Pymp, where each iteration calls the function `f(x)` on a different element of the array. Finally, it prints the resulting shared array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymp\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def f(x):\n",
    "    result = x*x\n",
    "    time.sleep(1) # emulate long runtime\n",
    "    print(f'Input: {x}| Result: {result}| Finish: {datetime.datetime.now()}\\n')\n",
    "    return result\n",
    "\n",
    "def test_pymp(n=10):\n",
    "    arr = [x for x in range(n)]\n",
    "    output = pymp.shared.array((n, ), dtype='int')\n",
    "    with pymp.Parallel(5) as p: # Parallel loop\n",
    "        for index in p.range(n):\n",
    "            output[index] = f(index)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pymp(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89a2a9",
   "metadata": {},
   "source": [
    "The final result is the same. However, there is no order of computing.\n",
    "\n",
    "We can use **pymp** to parallelize the code of our previous function `edistance_np`. Instead of using a loop we can use a pool of threads where each thread processes one row. For this we define a new function that works only with one row and the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymp\n",
    "\n",
    "\n",
    "def edistance_pymp(matrix1, matrix2):\n",
    "    # Shared memory \n",
    "    output = pymp.shared.array((matrix1.shape[0], matrix2.shape[0]), dtype='float64')\n",
    "    n_threads = 5\n",
    "    with pymp.Parallel(n_threads) as p: # Parallel loop\n",
    "        for index in p.range(matrix1.shape[0]):\n",
    "            # Complete the function\n",
    "    return output\n",
    "\n",
    "result_pymp = edistance_pymp(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_pymp = %timeit -n1 -r1 -o result_pymp = edistance_pymp(A, B)\n",
    "time_pymp = %timeit -o result_pymp = edistance_pymp(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9e080",
   "metadata": {},
   "source": [
    "It's a similar approach and outcome (in terms of performance) as multiprocessing. Nonetheless, PyMP may not have as extensive documentation and community support as more widely used libraries like NumPy, scikit-learn, or multiprocessing. Be prepared to refer to the library's documentation and possibly dive into its source code for more advanced usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3e923",
   "metadata": {},
   "source": [
    "## 6. Cython implementation <a name=\"cython\"></a>\n",
    "\n",
    "Cython is an optimizing static compiler for both the Python programming language and the extended Cython programming language (based on Pyrex). It makes writing C extensions for Python as easy as Python itself. \n",
    "\n",
    "https://cython.org/\n",
    "\n",
    "Cython allows to compile Python code and implement parallel operations with OpenMP.\n",
    "\n",
    "Unlike Python code, Cython code must be compiled before it can be executed. Python code is interpreted by the Python interpreter, whereas Cython code needs to go through a compilation process to generate optimized C code, which is then compiled into a Python extension module. This compiled module can be imported and used like any other Python module but typically offers improved performance due to its *closer-to-the-metal* nature and optional static typing.\n",
    "\n",
    "Normally, we would compile Python outside Jupyter notebook. However, for purposes of this workshop, we'll utilize Jupyter Notebook's magic functions to compile Cython code directly within the notebook. This approach is more straightforward and allows us to seamlessly integrate code development and compilation in one environment.\n",
    "\n",
    "In the following example we implement the Euclidean distance algorithm. To benefit from the usage of Cython, most variables are typed. Additionally, we use the ``prange()`` function to parallelize (single node - multicore) the execution of the code inside the second for loop which iterates over the rows of matrix2.\n",
    "\n",
    "**Install the Cython package:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b221da-73d4-43f2-9462-81616f9b5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbcbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12410ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-fopenmp --link-args=-fopenmp --force\n",
    "import warnings\n",
    "# Ignore all warnings for the duration of this cell\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# cython: profile=True\n",
    "# cython: linetrace=True\n",
    "# cython: binding=True\n",
    "# cython: linetrace=True\n",
    "# distutils: define_macros=CYTHON_TRACE_NOGIL=1\n",
    "#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
    "\n",
    "cimport cython\n",
    "import cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cython.parallel import prange\n",
    "from libc.math cimport sqrt\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cpdef edistance_cython(double[:,:] matrix1, double[:,:] matrix2):\n",
    "    cdef int i, j, k, I, J, K\n",
    "    I, J, K = matrix1.shape[0], matrix2.shape[0], matrix1.shape[1]\n",
    "    cdef np.ndarray distances = np.zeros((I, J))\n",
    "    cdef double[:,:] distances_mv = distances # memory view\n",
    "    cdef double diff, squared_diff\n",
    "    for i in range(I): # Iterate over matrix1 rows\n",
    "        for j in prange(J, nogil=True): #Iterate over rows of matrix2 in parallel\n",
    "            distances_mv[i, j] = 0.0\n",
    "            for k in range(K): # Iterate over columns of matrix1 and matrix2\n",
    "                diff = matrix1[i, k] - matrix2[j, k] # Get difference\n",
    "                squared_diff = diff ** 2 # Get squared difference\n",
    "                distances_mv[i, j] += squared_diff\n",
    "            distances_mv[i, j] = sqrt(distances_mv[i, j])\n",
    "    return distances\n",
    "\n",
    "# Restore the default warning behavior after the code block\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cython = edistance_cython(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4231f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_cython = %timeit -n1 -r1 -o result_cython = edistance_cython(A, B)\n",
    "time_cython = %timeit -o result_cython = edistance_cython(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e6524",
   "metadata": {},
   "source": [
    "## 7. Numba <a name=\"numba\"></a>\n",
    "\n",
    "Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code. \n",
    "\n",
    "Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library.\n",
    "\n",
    "You don't need to replace the Python interpreter, run a separate compilation step, or even have a C/C++ compiler installed. Just apply one of the Numba decorators to your Python function, and Numba does the rest. \n",
    "\n",
    "For more information: https://numba.pydata.org/\n",
    "\n",
    "------------------------\n",
    "\n",
    "By definition, a decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it.\n",
    "\n",
    "The following two pieces of code are equivalent:\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: center;\" width=\"50%\">With decorators</th>\n",
    "<th style=\"text-align: center;\" width=\"50%\">Without decorators</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">\n",
    "    \n",
    "```python\n",
    "@dec2\n",
    "@dec1\n",
    "def func(arg1, arg2, ...):\n",
    "    pass\n",
    "```\n",
    "</td>\n",
    "<td style=\"text-align: left;\">\n",
    "\n",
    "```python\n",
    "def func(arg1, arg2, ...):\n",
    "    pass\n",
    "func = dec2(dec1(func))\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "More info: https://peps.python.org/pep-0318/\n",
    "\n",
    "------------------------\n",
    "\n",
    "The code is quite similar to the Numpy implementation presented previously. Main additions are the decorators with parameters `parallel` and `nopython` set to true. \n",
    "\n",
    "- The `@jit` decorator stands for \"Just-In-Time\" compilation and is used to instruct Numba to compile a Python function to machine code for performance improvements.\n",
    "- The parallel option is used with `@jit` to enable parallel execution of loops within the decorated function.\n",
    "- Nopython mode is more aggressive in optimizing code. It aims to generate code that does not rely on the Python C-API (Python's internal C interface), resulting in potentially faster code.\n",
    "\n",
    "**Install Numba package:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19222425-78fb-4290-ac34-6afccd66dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba #Do it once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit, prange\n",
    "\n",
    "@jit(parallel=True, nopython=True) # Decorator\n",
    "def edistance_numba(matrix1, matrix2):\n",
    "    output = np.zeros((matrix1.shape[0], matrix2.shape[0]))\n",
    "    for i in prange(matrix1.shape[0]): # Note the use of prange\n",
    "        for j in range(matrix2.shape[0]):\n",
    "            diff = matrix1[i] - matrix2[j]\n",
    "            squared_diff = diff * diff\n",
    "            output[i, j] = np.sqrt(np.sum(squared_diff))\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "result_numba = edistance_numba(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_numba = %timeit -n1 -r1 -o results_numba = edistance_numba(A, B)\n",
    "time_numba = %timeit -o results_numba = edistance_numba(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a75aa",
   "metadata": {},
   "source": [
    "The execution speed is much faster than the previous versions. Mainly because it takes advantages from compilation and parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38e304",
   "metadata": {},
   "source": [
    "## Result validation\n",
    "\n",
    "We can check the results of each version with the ``np.allclose()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76861065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify a tolerance (e.g., 0.01) for floating-point comparisons\n",
    "tolerance = 0.01\n",
    "print('Python and Numpy results are equal:', np.allclose(result_python, result_numpy, atol=tolerance))\n",
    "print('Numpy and Multiprocessing results are equal:', np.allclose(result_numpy, result_multiproc, atol=tolerance))\n",
    "print('Multiprocessing and PyMP results are equal:', np.allclose(result_multiproc, result_pymp, atol=tolerance))\n",
    "print('PyMP and Cython results are equal:', np.allclose(result_pymp, result_cython, atol=tolerance))\n",
    "print('Cython and Numba results are equal:', np.allclose(result_cython, result_numba, atol=tolerance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b136143",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "Using matplotlib we generate a grouped bar plot with the results of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "# Create arrays for the plot\n",
    "version = ['Python', 'Numpy', 'PyMP', 'Cython', 'Numba']\n",
    "c = ['blue', 'blue', 'orange', 'orange', 'orange', 'orange']\n",
    "x_pos = np.arange(len(version))\n",
    "time = [time_python.best, time_numpy.best, time_pymp.best, time_cython.best, time_numba.best]\n",
    "\n",
    "colors = {'CPU Singlecore':'blue', 'CPU Multicore':'orange'}\n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, time, align='center', color=c)\n",
    "ax.legend(handles, labels)\n",
    "ax.set_ylabel('Time in seconds')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(version)\n",
    "#ax.set_title('Benchmark of Accelerating Soft-supervised training algorithm')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "# Text on the top of each bar\n",
    "for index,data in enumerate(time):\n",
    "    plt.text(x=index-0.1 , y =data+0.1 , s=f\"{round(data,2)}\" , fontdict=dict(fontsize=14))\n",
    "#plt.savefig('bm_results1.eps') # Uncomment if want to save file\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f30506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the plot\n",
    "version = ['Numpy', 'PyMP', 'Cython', 'Numba']\n",
    "c = ['blue', 'orange', 'orange', 'orange', 'orange']\n",
    "x_pos = np.arange(len(version))\n",
    "time = [time_numpy.best, time_pymp.best, time_cython.best, time_numba.best]\n",
    "\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, time, align='center', color=c)\n",
    "ax.legend(handles, labels)\n",
    "ax.set_ylabel('Time in seconds')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(version)\n",
    "#ax.set_title('Benchmark of Accelerating Euclidean-distance algorithm')\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Save the figure and show\n",
    "plt.tight_layout()\n",
    "# Text on the top of each bar\n",
    "for index,data in enumerate(time):\n",
    "    plt.text(x=index-0.1 , y =data+0.025 , s=f\"{round(data,2)}\" , fontdict=dict(fontsize=14))\n",
    "#plt.savefig('bm_results2.eps') # Uncomment if want to save file\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709eeb3d",
   "metadata": {},
   "source": [
    "## 8. Benchmarking: More cores equals more speed? <a name=\"benchmarking\"></a>\n",
    "\n",
    "\n",
    "We've carried out a benchmark involving a range of matrix sizes and experimented with different thread configurations. The benchmark script has been run (please note, it takes some time to complete), and the results are tabulated below for your reference. The row index corresponds to the matrix sizes, while the column names specify the library used and, where applicable, the number of threads employed.\n",
    "\n",
    "\n",
    "We integrated Numba into our benchmark and observed that, compared to other implementations, it consistently delivers the fastest execution times. Furthermore, transitioning to Numba from existing Numpy code demands minimal effort—typically involving the addition of decorators and parallel loops to your existing codebase. This streamlined integration process makes it an attractive choice for optimizing performance in computationally intensive tasks.\n",
    "\n",
    "To do this we are going to install pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3556993-b236-4b0d-b474-a59562449aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee36ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_mp = pd.read_csv(\"exp_mp_results_tmp.csv\", index_col=0)\n",
    "#df_mp = df_mp.round(2)\n",
    "\n",
    "\n",
    "# Display the rounded DataFrame with four decimal places\n",
    "#pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Function to highlight the minimum value in each row with green\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: lightgreen' if v else '' for v in is_min]\n",
    "\n",
    "# Function to highlight the maximum value in each row with light red\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: lightcoral' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the functions when displaying the DataFrame\n",
    "df_mp.style.apply(highlight_min, axis=1).apply(highlight_max, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a9224",
   "metadata": {},
   "source": [
    "We've marked the fastest execution in green and the slowest in red.\n",
    "\n",
    "As we can see in this case, having more cores doesn't necessarily translate to greater speed. Utilizing parallelization techniques introduces overhead, as evidenced by the fact that using 128 cores with smaller matrix sizes results in the poorest execution speed.\n",
    "\n",
    "After matrices are of size 3,000 or larger NumPy is always slower than using multiprocessing with any number of threads.\n",
    "\n",
    "After matrices are of size 4,000 or larger using 25 threads yields the fastest execution speed.\n",
    "\n",
    "\n",
    "## 9. Conclusions / take away<a name=\"conclusions\"></a>\n",
    "\n",
    "Important note: keep in mind that the results presented in this notebook are for a specific use case. Different scenarios may yield varying results, and alternate approaches, such as multiprocessing or PyMP, may prove more effective in certain contexts. There is no one-size-fits-all solution. \n",
    "\n",
    "\n",
    "- Starting with Numba is a recommended practice due to its minimal time investment requirement and potential for performance optimization.\n",
    "- Cython also offers a very good speedup, compared to Numpy. However, we can observe that Numpy and Numba's implementation is not only simpler but also more readable than its Cython counterpart.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: center;\" width=\"50%\">Numpy</th>\n",
    "<th style=\"text-align: center;\" width=\"50%\">Cython</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left;\">\n",
    "    \n",
    "```python\n",
    "def edistance_3(matrix1, matrix2):\n",
    "    output = np.zeros((A.shape[0], B.shape[0]))\n",
    "    for i, row1 in enumerate(matrix1):\n",
    "        output[i] = np.sqrt(np.sum((row1 - matrix2)**2, axis=1))\n",
    "    return output\n",
    "```\n",
    "</td>\n",
    "<td style=\"text-align: left;\">\n",
    "\n",
    "```python\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "cpdef edistance_5(double[:,:] matrix1, double[:,:] matrix2):\n",
    "    cdef int i, j, k\n",
    "    cdef int I = matrix1.shape[0]\n",
    "    cdef int J = matrix2.shape[0]\n",
    "    cdef int K = matrix1.shape[1]\n",
    "    cdef np.ndarray distances = np.zeros((matrix1.shape[0], matrix2.shape[0]))\n",
    "    cdef double[:,:] distances_mv = distances # memory view\n",
    "    cdef double res_1, res_2\n",
    "\n",
    "    for i in range(I): # Iterate over matrix1 rows\n",
    "        for j in prange(J, nogil=True): #Iterate over rows of matrix2\n",
    "            distances_mv[i, j] = 0.0\n",
    "            for k in range(K): # Iterate over columns of matrix1 and matrix2\n",
    "                res_1 = matrix1[i, k] - matrix2[j, k] # Get difference\n",
    "                res_2 = res_1 ** 2 # Get squared difference\n",
    "                distances_mv[i, j] += res_2\n",
    "            distances_mv[i, j] = sqrt(distances_mv[i, j])\n",
    "    return distances\n",
    "```\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- Multiprocessing and PyMP are programed more or less similar, but with some important differences. For example, multiprocessing spawns processes and PyMP uses threads. Additionally, multiprocessing offers a more extensive documentation compared to PyMP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e91f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
